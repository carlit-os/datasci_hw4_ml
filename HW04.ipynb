{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Obtain the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = datasets.fetch_california_housing()\n",
    "cal_data=cal.data.astype(int)\n",
    "cal_target=cal.target.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Regression. (3%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.a Linear model vs. GBT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nsvm_model = SVR(kernel='linear')\\n#linreg_performance = cross_val(5,cal.data,cal.train,svm_model)\\n\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##cross validation k=5\n",
    "##break data into 5 pieces and average performance across these\n",
    "\n",
    "##.fit(trainingvalues,target values)\n",
    "\n",
    "'''\n",
    "def cross_val(folds,train,target,reg):\n",
    "    ## assumes train and target are arrays with same num rows\n",
    "    ## collects model scores and returns averages over folds\n",
    "\n",
    "    step = int(len(train)/folds)\n",
    "    end = step\n",
    "    start = 0\n",
    "    total = 0\n",
    "\n",
    "    for partition in range(folds):\n",
    "        reg.fit(train[start:end],target[start:end])\n",
    "        total += reg.score(train[start:end],target[start:end])\n",
    "\n",
    "        #update range\n",
    "        start = end\n",
    "        end = end + step\n",
    "\n",
    "    return total/folds\n",
    "'''\n",
    "\n",
    "'''\n",
    "svm_model = SVR(kernel='linear')\n",
    "#linreg_performance = cross_val(5,cal.data,cal.train,svm_model)\n",
    "'''\n",
    "#lin_model = sklearn.linear_model.LinearRegression().fit(cal.data,cal.target)\n",
    "#cross_val(5,cal.data,cal.target,sklearn.linear_model.LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a linear regression model, 47.06% of the data fit the model\n",
      "\n",
      "\n",
      "With a gradient boosting tree regression model, 49.07% of the data fit the model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "linreg = sklearn.linear_model.LinearRegression()\n",
    "##fit on the first half of the data\n",
    "\n",
    "half = len(cal_data)//2\n",
    "quarter = len(cal_data)//4\n",
    "threeq = quarter*3\n",
    "\n",
    "\n",
    "linreg.fit(cal_data[:threeq],cal_target[:threeq])\n",
    "\n",
    "##score on the second half, use r2 scoring for regression\n",
    "linear_performance = cross_val_score(linreg,cal_data[threeq:],cal_target[threeq:], cv=5, scoring='r2').mean()\n",
    "\n",
    "\n",
    "print(\"With a linear regression model, %.2f%% of the data fit the model\"%(linear_performance*100))\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "gbtreg = GradientBoostingRegressor()\n",
    "gbtreg.fit(cal_data[:threeq],cal_target[:threeq])\n",
    "gbt_performance = cross_val_score(gbtreg,cal_data[threeq:],cal_target[threeq:], cv=5, scoring='r2').mean()\n",
    "\n",
    "print(\"With a gradient boosting tree regression model, %.2f%% of the data fit the model\"%(gbt_performance*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.b Search for better parameters in GBT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss funcitons\n",
      "lad 41.362955921305954\n",
      "huber 39.588201305165796\n",
      "quantile -39.700995382726944\n"
     ]
    }
   ],
   "source": [
    "##Loss functions\n",
    "print(\"Loss funcitons\")\n",
    "gbtreg = GradientBoostingRegressor(loss='lad')\n",
    "gbtreg.fit(cal_data[:threeq],cal_target[:threeq])\n",
    "gbt_performance = cross_val_score(gbtreg,cal_data[threeq:],cal_target[threeq:], cv=5, scoring='r2').mean()\n",
    "print(\"lad\",gbt_performance*100)\n",
    "\n",
    "gbtreg = GradientBoostingRegressor(loss='huber')\n",
    "gbtreg.fit(cal_data[:threeq],cal_target[:threeq])\n",
    "gbt_performance = cross_val_score(gbtreg,cal_data[threeq:],cal_target[threeq:], cv=5, scoring='r2').mean()\n",
    "print(\"huber\",gbt_performance*100)\n",
    "\n",
    "gbtreg = GradientBoostingRegressor(loss='quantile')\n",
    "gbtreg.fit(cal_data[:threeq],cal_target[:threeq])\n",
    "gbt_performance = cross_val_score(gbtreg,cal_data[threeq:],cal_target[threeq:], cv=5, scoring='r2').mean()\n",
    "print(\"quantile\",gbt_performance*100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'mean_fit_time': array([0.52866559, 0.7756783 , 0.84914603, 2.11907358, 1.0102078 ,\n",
      "       1.90695443, 1.5002152 , 3.81214452, 1.12298579, 1.72829881,\n",
      "       2.23660517, 5.56480718, 1.28749242, 2.03304191, 3.04795637,\n",
      "       5.65920849, 0.52781248, 0.85498157, 1.04348817, 2.12501626,\n",
      "       0.73259826, 1.18444171, 1.47654781, 3.03627362, 1.42526813,\n",
      "       1.7073205 , 1.97199373, 3.83164692, 1.18611269, 1.8744092 ,\n",
      "       2.68789358, 5.20332241, 0.50249557, 0.78807402, 0.988411  ,\n",
      "       2.10444021, 0.7345314 , 1.15992169, 1.39695659, 2.74923468,\n",
      "       0.91269536, 1.44883399, 1.79226146, 3.61495118, 1.33517547,\n",
      "       2.42726793, 2.69112382, 4.71254826, 0.47900448, 0.93680882,\n",
      "       1.01296115, 2.23046966, 0.83877578, 1.47666206, 1.59478779,\n",
      "       4.29800377, 0.93090911, 1.86262474, 2.17140617, 3.62913327,\n",
      "       1.24949136, 2.21975989, 2.35414834, 4.47773137]), 'std_fit_time': array([0.12385547, 0.18014913, 0.01388024, 0.3363805 , 0.28080623,\n",
      "       0.36209556, 0.10622644, 0.64801151, 0.24235514, 0.12365978,\n",
      "       0.58799192, 0.61649575, 0.01516877, 0.03575597, 0.36308132,\n",
      "       0.63200888, 0.01899847, 0.04543052, 0.01923155, 0.09853943,\n",
      "       0.00568124, 0.02098255, 0.02276051, 0.20819267, 0.22404435,\n",
      "       0.14765526, 0.02838562, 0.07452186, 0.01801732, 0.0214071 ,\n",
      "       0.37523086, 0.44727932, 0.01446839, 0.01545971, 0.02852212,\n",
      "       0.18651693, 0.02735306, 0.02388935, 0.03002656, 0.07073093,\n",
      "       0.02069655, 0.02546135, 0.02495339, 0.10006619, 0.26580207,\n",
      "       0.24780381, 0.39860211, 0.27181692, 0.01024725, 0.14079272,\n",
      "       0.05056941, 0.23836668, 0.11231645, 0.1711509 , 0.16237223,\n",
      "       0.76182551, 0.00958968, 0.27547287, 0.28498807, 0.0780755 ,\n",
      "       0.14398837, 0.26524318, 0.07402517, 0.11891989]), 'mean_score_time': array([0.00284014, 0.00374823, 0.00406284, 0.00728998, 0.00665956,\n",
      "       0.00763898, 0.00586996, 0.01379728, 0.00528021, 0.00802798,\n",
      "       0.00817342, 0.02033877, 0.00646858, 0.00883846, 0.01237164,\n",
      "       0.0212574 , 0.00277157, 0.00384154, 0.00442986, 0.00757103,\n",
      "       0.00378404, 0.00495677, 0.00621591, 0.01392169, 0.00677519,\n",
      "       0.00685897, 0.00761313, 0.01280556, 0.00548058, 0.00780387,\n",
      "       0.01269317, 0.01876602, 0.00249095, 0.00344458, 0.00411477,\n",
      "       0.00767794, 0.00397701, 0.00483642, 0.0055892 , 0.00906634,\n",
      "       0.00372419, 0.00556312, 0.00613284, 0.01162925, 0.00573564,\n",
      "       0.00806704, 0.01054425, 0.01700249, 0.00233979, 0.0042582 ,\n",
      "       0.0041348 , 0.00796404, 0.00415964, 0.00555763, 0.00581694,\n",
      "       0.0124197 , 0.0042232 , 0.00636816, 0.00710621, 0.01285396,\n",
      "       0.006463  , 0.0075676 , 0.00902057, 0.01609478]), 'std_score_time': array([0.00056634, 0.00133441, 0.00038125, 0.00074103, 0.00221409,\n",
      "       0.00077498, 0.0002614 , 0.00310857, 0.0013529 , 0.00075149,\n",
      "       0.00116352, 0.00684604, 0.00088637, 0.00140106, 0.00210583,\n",
      "       0.0034277 , 0.00028923, 0.00033504, 0.00050038, 0.00065555,\n",
      "       0.00043868, 0.00055872, 0.00076127, 0.00590335, 0.00184282,\n",
      "       0.00051962, 0.0006668 , 0.0015178 , 0.00065993, 0.00080264,\n",
      "       0.00519304, 0.00486263, 0.00024824, 0.00041173, 0.00053377,\n",
      "       0.00055087, 0.00050912, 0.0004215 , 0.00056477, 0.00090445,\n",
      "       0.00012156, 0.00057252, 0.00021253, 0.0025775 , 0.00097048,\n",
      "       0.00049949, 0.00230999, 0.00265949, 0.0002294 , 0.00153622,\n",
      "       0.00028488, 0.00104669, 0.00098454, 0.00068997, 0.00112134,\n",
      "       0.00170223, 0.00072281, 0.00086945, 0.00066783, 0.0013281 ,\n",
      "       0.00227687, 0.00066309, 0.00116243, 0.00103658]), 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
      "                   0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,\n",
      "                   0.15, 0.15, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
      "                   0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.25, 0.25,\n",
      "                   0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
      "                   0.25, 0.25, 0.25, 0.25, 0.25],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 2, 2,\n",
      "                   2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 2, 2, 2, 2,\n",
      "                   3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 2, 2, 2, 2, 3, 3,\n",
      "                   3, 3, 4, 4, 4, 4, 5, 5, 5, 5],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[50, 80, 100, 200, 50, 80, 100, 200, 50, 80, 100, 200,\n",
      "                   50, 80, 100, 200, 50, 80, 100, 200, 50, 80, 100, 200,\n",
      "                   50, 80, 100, 200, 50, 80, 100, 200, 50, 80, 100, 200,\n",
      "                   50, 80, 100, 200, 50, 80, 100, 200, 50, 80, 100, 200,\n",
      "                   50, 80, 100, 200, 50, 80, 100, 200, 50, 80, 100, 200,\n",
      "                   50, 80, 100, 200],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 80}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 200}, {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 80}, {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 80}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 200}, {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 80}, {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}, {'learning_rate': 0.15, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.15, 'max_depth': 2, 'n_estimators': 80}, {'learning_rate': 0.15, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.15, 'max_depth': 2, 'n_estimators': 200}, {'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 50}, {'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 80}, {'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 100}, {'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 200}, {'learning_rate': 0.15, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.15, 'max_depth': 4, 'n_estimators': 80}, {'learning_rate': 0.15, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.15, 'max_depth': 4, 'n_estimators': 200}, {'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 50}, {'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 80}, {'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 100}, {'learning_rate': 0.15, 'max_depth': 5, 'n_estimators': 200}, {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 80}, {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators': 200}, {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50}, {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 80}, {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100}, {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200}, {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 80}, {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 200}, {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 50}, {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 80}, {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 100}, {'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 200}, {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 80}, {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 200}, {'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 50}, {'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 80}, {'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 100}, {'learning_rate': 0.25, 'max_depth': 3, 'n_estimators': 200}, {'learning_rate': 0.25, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.25, 'max_depth': 4, 'n_estimators': 80}, {'learning_rate': 0.25, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.25, 'max_depth': 4, 'n_estimators': 200}, {'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 50}, {'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 80}, {'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 100}, {'learning_rate': 0.25, 'max_depth': 5, 'n_estimators': 200}], 'split0_test_score': array([0.37674918, 0.36678539, 0.39352498, 0.42653929, 0.39366234,\n",
      "       0.43139705, 0.42757448, 0.42928457, 0.42719327, 0.43106211,\n",
      "       0.44091564, 0.43144706, 0.43574108, 0.4310697 , 0.42985875,\n",
      "       0.41952421, 0.37066152, 0.39898794, 0.39971461, 0.43110883,\n",
      "       0.42947855, 0.44761263, 0.44169689, 0.43125879, 0.43436169,\n",
      "       0.4483401 , 0.45166663, 0.44370197, 0.43904619, 0.43468972,\n",
      "       0.42599655, 0.42986263, 0.40293685, 0.40226522, 0.41054218,\n",
      "       0.41358323, 0.43871851, 0.44071974, 0.44717439, 0.43972472,\n",
      "       0.42357342, 0.41724803, 0.42290296, 0.42018272, 0.43037334,\n",
      "       0.42931198, 0.4211839 , 0.41821792, 0.39753623, 0.41652642,\n",
      "       0.41645834, 0.41508815, 0.42303665, 0.43717414, 0.43128573,\n",
      "       0.42505751, 0.44870577, 0.42971563, 0.42890349, 0.41177482,\n",
      "       0.41464255, 0.41682483, 0.41167727, 0.39180471]), 'split1_test_score': array([0.55018513, 0.58276952, 0.58665459, 0.58877276, 0.59146202,\n",
      "       0.60434362, 0.60332005, 0.61069767, 0.60618984, 0.60786077,\n",
      "       0.60912367, 0.60947706, 0.61515472, 0.61236795, 0.61101725,\n",
      "       0.60536423, 0.57979305, 0.58767834, 0.58699421, 0.59051166,\n",
      "       0.60418349, 0.60935987, 0.60884017, 0.60899162, 0.60714531,\n",
      "       0.60800489, 0.60755107, 0.60540177, 0.61020881, 0.6113597 ,\n",
      "       0.60718869, 0.59774736, 0.58753747, 0.5931277 , 0.59348762,\n",
      "       0.59161853, 0.60374176, 0.60898563, 0.60856764, 0.60897877,\n",
      "       0.60927297, 0.6067133 , 0.60472104, 0.59819525, 0.61184252,\n",
      "       0.60838251, 0.60765442, 0.59992291, 0.58517795, 0.58343438,\n",
      "       0.5857823 , 0.59076205, 0.60987854, 0.60867015, 0.60665413,\n",
      "       0.60385563, 0.60592101, 0.60994094, 0.60303274, 0.58858314,\n",
      "       0.59481069, 0.59006634, 0.58772141, 0.57776054]), 'split2_test_score': array([0.56412151, 0.58960447, 0.59375334, 0.60105485, 0.60316283,\n",
      "       0.61211408, 0.61450694, 0.61929713, 0.62073458, 0.62592884,\n",
      "       0.62754838, 0.63152716, 0.63161567, 0.63455876, 0.63631989,\n",
      "       0.63397624, 0.5937753 , 0.60351602, 0.60424223, 0.61217175,\n",
      "       0.60991299, 0.61428197, 0.61583653, 0.62475105, 0.62254166,\n",
      "       0.62486392, 0.62533605, 0.62643464, 0.62581212, 0.62884938,\n",
      "       0.62853691, 0.62595675, 0.59427002, 0.60137028, 0.6031706 ,\n",
      "       0.61030188, 0.61752614, 0.62085923, 0.62180564, 0.62616851,\n",
      "       0.62856523, 0.62769047, 0.62895189, 0.62727972, 0.63251322,\n",
      "       0.63357435, 0.63314741, 0.62859003, 0.60354624, 0.60740399,\n",
      "       0.60859184, 0.61557724, 0.61559808, 0.62172838, 0.62463913,\n",
      "       0.62437473, 0.62666584, 0.62774942, 0.62618482, 0.62415806,\n",
      "       0.63283707, 0.63019456, 0.62764545, 0.62230465]), 'split3_test_score': array([0.39268613, 0.45338547, 0.47066757, 0.50072305, 0.46701345,\n",
      "       0.50650852, 0.51611857, 0.52297469, 0.49222163, 0.51960613,\n",
      "       0.52249301, 0.52413409, 0.51251792, 0.52347616, 0.52674861,\n",
      "       0.52530316, 0.44995039, 0.47788533, 0.48855432, 0.50508553,\n",
      "       0.49882619, 0.51204435, 0.5146877 , 0.51525277, 0.51404667,\n",
      "       0.52562701, 0.52482412, 0.5180585 , 0.51717985, 0.51956576,\n",
      "       0.51885743, 0.51848021, 0.45704986, 0.48385011, 0.48609036,\n",
      "       0.51226653, 0.51639108, 0.52496867, 0.52721862, 0.52908206,\n",
      "       0.52037785, 0.5256178 , 0.52552124, 0.51412603, 0.52598556,\n",
      "       0.52806275, 0.52803373, 0.51330601, 0.47734203, 0.48631274,\n",
      "       0.49587255, 0.5157971 , 0.51641942, 0.52243747, 0.52402176,\n",
      "       0.51598331, 0.52639774, 0.52663675, 0.52749199, 0.52453101,\n",
      "       0.51589103, 0.51498073, 0.51491765, 0.49549545]), 'split4_test_score': array([0.59445687, 0.62345823, 0.62868085, 0.64775981, 0.62041651,\n",
      "       0.63892153, 0.64412605, 0.64358515, 0.6396375 , 0.64275241,\n",
      "       0.64262802, 0.62511842, 0.63082257, 0.63356042, 0.62664199,\n",
      "       0.61156589, 0.62865711, 0.64435383, 0.64743777, 0.64703892,\n",
      "       0.64387227, 0.64547409, 0.64516857, 0.63928562, 0.64944768,\n",
      "       0.64700908, 0.64185606, 0.61143592, 0.63549254, 0.62377714,\n",
      "       0.62236992, 0.59668868, 0.63298367, 0.64044859, 0.64206135,\n",
      "       0.64256207, 0.64422125, 0.63673418, 0.63594204, 0.61743668,\n",
      "       0.63893377, 0.62909113, 0.62810347, 0.59214887, 0.63446458,\n",
      "       0.61707687, 0.60709623, 0.57113973, 0.65076698, 0.64778951,\n",
      "       0.64822664, 0.63923596, 0.63927738, 0.64056943, 0.63969203,\n",
      "       0.61559189, 0.63219539, 0.63035405, 0.62613793, 0.58037584,\n",
      "       0.61715078, 0.59798368, 0.58802034, 0.55224006]), 'mean_test_score': array([0.49563976, 0.52320061, 0.53465626, 0.55296995, 0.53514343,\n",
      "       0.55865696, 0.56112922, 0.56516784, 0.55719536, 0.56544205,\n",
      "       0.56854174, 0.56434076, 0.56517039, 0.5670066 , 0.5661173 ,\n",
      "       0.55914675, 0.52456747, 0.54248429, 0.54538863, 0.55718334,\n",
      "       0.5572547 , 0.56575458, 0.56524597, 0.56390797, 0.5655086 ,\n",
      "       0.570769  , 0.57024679, 0.56100656, 0.5655479 , 0.56364834,\n",
      "       0.5605899 , 0.55374712, 0.53495558, 0.54421238, 0.54707042,\n",
      "       0.55406645, 0.56411975, 0.56645349, 0.56814167, 0.56427815,\n",
      "       0.56414465, 0.56127214, 0.56204012, 0.55038652, 0.56703584,\n",
      "       0.56328169, 0.55942314, 0.54623532, 0.54287389, 0.54829341,\n",
      "       0.55098633, 0.5552921 , 0.56084201, 0.56611591, 0.56525856,\n",
      "       0.55697261, 0.56797715, 0.56487936, 0.5623502 , 0.54588457,\n",
      "       0.55506642, 0.55001003, 0.54599642, 0.52792108]), 'std_test_score': array([0.09183048, 0.0973133 , 0.08837365, 0.07911203, 0.08913867,\n",
      "       0.07784841, 0.0792219 , 0.07924234, 0.08289333, 0.07949062,\n",
      "       0.07619234, 0.07684079, 0.07841919, 0.07927685, 0.07838869,\n",
      "       0.07889628, 0.09790709, 0.09045331, 0.08950962, 0.07850795,\n",
      "       0.08027259, 0.07409971, 0.07572053, 0.07922037, 0.07984985,\n",
      "       0.07369119, 0.07166421, 0.0698567 , 0.0759747 , 0.07578472,\n",
      "       0.0780538 , 0.0715337 , 0.08875901, 0.08799289, 0.08565465,\n",
      "       0.08232475, 0.07594984, 0.07380828, 0.07130042, 0.07128354,\n",
      "       0.0817995 , 0.08135095, 0.0791909 , 0.0751135 , 0.07899788,\n",
      "       0.07623296, 0.07762684, 0.07450863, 0.09222705, 0.08465526,\n",
      "       0.08380947, 0.08143217, 0.08064565, 0.07613242, 0.07801177,\n",
      "       0.07649706, 0.07065775, 0.07746206, 0.07592354, 0.07427106,\n",
      "       0.08091468, 0.0765518 , 0.07639512, 0.07945153]), 'rank_test_score': array([64, 63, 60, 45, 58, 36, 30, 18, 38, 14,  3, 20, 17,  7,  9, 35, 62,\n",
      "       57, 54, 39, 37, 11, 16, 24, 13,  1,  2, 31, 12, 25, 33, 44, 59, 55,\n",
      "       50, 43, 23,  8,  4, 21, 22, 29, 28, 47,  6, 26, 34, 51, 56, 49, 46,\n",
      "       41, 32, 10, 15, 40,  5, 19, 27, 53, 42, 48, 52, 61], dtype=int32)}\n",
      "\n",
      "\n",
      "0.5707689994618337\n",
      "\n",
      "{'learning_rate': 0.15, 'max_depth': 4, 'n_estimators': 80}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\")\n",
    "parameters={'learning_rate':[.1,.15,.2,.25],'max_depth':[2,3,4,5],'n_estimators':[50,80,100,200]}\n",
    "gs_model=GridSearchCV(GradientBoostingRegressor(),parameters,scoring=\"r2\",cv=5)\n",
    "gs_model.fit(cal_data,cal_target)\n",
    "print(\"\")\n",
    "print(gs_model.cv_results_)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(gs_model.best_score_)\n",
    "print(\"\")\n",
    "print(gs_model.best_params_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Classification. (3%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.a Logistic regression vs. GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Library/Python/3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Library/Python/3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Library/Python/3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Library/Python/3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Library/Python/3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3315891472868217\n",
      "0.4775193798449612\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(cal_data[:threeq],cal_target[:threeq])\n",
    "\n",
    "##score on the second half, use r2 scoring for regression\n",
    "logistic_performance = cross_val_score(logreg,cal_data[threeq:],cal_target[threeq:], cv=5, scoring='accuracy').mean()\n",
    "print(logistic_performance)\n",
    "\n",
    "gbt_class=GradientBoostingClassifier()\n",
    "gbt_class.fit(cal_data[:threeq],cal_target[:threeq])\n",
    "gbt_class_performance = cross_val_score(gbt_class,cal_data[threeq:],cal_target[threeq:], cv=5, scoring='accuracy').mean()\n",
    "print(gbt_class_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.b Search for optimal parameters in GBT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.c SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}